[Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)

#LlaMA #LLM 

### Why Llama 2?

The pretrained models come with significant improvements over the [[Articles/LLaMA - Open and Efficient Foundation Language Models|Llama 1]] models, including being trained on 40% more tokens, having a much longer context length (4k tokens), and using grouped-query attention for fast inference of the 70B model.
The fine-tuned models (Llama 2-Chat), which have been optimized for dialogue applications usingÂ [Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf). Across a wide range of helpfulness and safety benchmarks, the Llama 2-Chat models perform better than most open models and achieve comparable performance to ChatGPT according to human evaluations.